services:
  # This is the reverse proxy that exposes a single port and load balances
  # traffic across all webapp instances.
  proxy:
    image: nginx:alpine
    volumes:
      - ./nginx.conf:/etc/nginx/conf.d/default.conf:ro
    ports:
      - "8080:80"
    depends_on:
      - webapp
    networks:
      - web-net
      
  # The autoscaler service itself
  autoscaler:
    build:
      context: .
      dockerfile: Dockerfile
    depends_on:
      - webapp # Autoscaler needs the webapp to be defined
    # The command provides the arguments to the go-autoscale executable
    # The command provides the arguments to the go-autoscale executable
    command:
      - "--service"
      - "webapp"
      - "--cpu-up"
      - "20" # Scale up when CPU is > 20%
      - "--cpu-down"
      - "10" # Scale down when CPU is < 10%
      - "--metric"
      - "cpu"
      - "--poll"
      - "2s"
      - "--initial-grace-period"
      - "5s" # Wait 5 seconds after startup before checking metrics
      - "--project-name"
      - "${PROJECT_NAME}" # Pass the project name from the Makefile
      - "--max"
      - "10"
      #- "--dry-run"
    volumes:
      # Mount the docker socket to allow the autoscaler to interact with the Docker daemon
      - /var/run/docker.sock:/var/run/docker.sock
      # Mount this docker-compose.yml file to allow the autoscaler to run compose commands
      - ./docker-compose.yml:/app/docker-compose.yml:ro
    environment:
      # This ensures the compose command inside the container targets the correct project.
      # The project name is derived from the directory name by the Makefile.
      - COMPOSE_PROJECT_NAME=${PROJECT_NAME}
    networks:
      - web-net

  # This is the sample application we want to autoscale
  webapp:
    image: nginx:alpine
    deploy:
      resources:
        limits:
          cpus: '0.5' # Limit CPU to make it easier to stress
          memory: 128M
    networks:
      - web-net

networks:
  web-net:
